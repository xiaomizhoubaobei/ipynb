{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct3qhBX7rKDC"
      },
      "source": [
        "版权所有 2025 Google LLC。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "z11nEz9krMGQ"
      },
      "outputs": [],
      "source": [
        "#@title 根据 Apache 许可证 2.0 版（“许可证”）授权；\n",
        "# 除非遵守许可证，否则您不得使用此文件。\n",
        "# 您可以通过以下网址获取许可证副本：\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0  \n",
        "#\n",
        "# 除非适用法律要求或书面同意，软件\n",
        "# 根据许可证分发是基于“原样”基础，\n",
        "# 不提供任何形式的担保或条件。\n",
        "# 请参阅许可证以了解特定语言下的权限和\n",
        "# 限制。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkOtOq0jDE0c"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "      <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Convert_Gemma_3_270M_to_LiteRT_for_MediaPipe_LLM_Inference_API.ipynb  \"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png  \" />在 Google Colab 中运行</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDkoc9st4TYk"
      },
      "source": [
        "# 将 Gemma 3 270M 转换为 LiteRT 以用于 MediaPipe LLM 推理 API\n",
        "\n",
        "本笔记本将 Gemma 3 270M 模型转换为适用于 [MediaPipe LLM 推理 API](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference) 的格式，该库支持在移动设备或网页浏览器中运行推理。整个过程大约需要 15 分钟：\n",
        "\n",
        "1. 设置 Colab 环境\n",
        "2. 从 Hugging Face 加载模型\n",
        "3. 使用 AI Edge Torch 转换器转换模型\n",
        "4. 使用 MediaPipe Task 打包器打包模型\n",
        "5. 下载模型\n",
        "\n",
        "Gemma 3 270M 专为特定任务微调而设计，并针对移动设备、网页和边缘设备的高效性能进行了优化。你可以使用[这个笔记本](https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Fine_tune_Gemma_3_270M_for_emoji_generation.ipynb)微调自己的模型，并在转换后运行在[网页演示应用](https://github.com/google-gemini/gemma-cookbook/tree/main/Demos/Emoji-Gemma-on-Web/app-mediapipe)中。\n",
        "\n",
        "## 设置开发环境\n",
        "\n",
        "第一步是使用 pip 安装所需的包。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEAwOswpsqeq"
      },
      "outputs": [],
      "source": [
        "%pip uninstall -y tensorflow\n",
        "%pip install -U tf-nightly==2.21.0.dev20250819 ai-edge-torch==0.6.0 protobuf transformers\n",
        "%pip install -U jax jaxlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8U1zoAY-4g4"
      },
      "source": [
        "重启会话运行时以确保使用新安装的包。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtibyxZmZjHG"
      },
      "source": [
        "## 加载模型\n",
        "要访问 Hugging Face 上的模型，请提供你的[访问令牌](https://huggingface.co/settings/tokens)。你可以将其添加到环境变量变量名称为 `HF_TOKEN`，值为你的令牌。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCQO6oy41gZs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "login(hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk2ni3i11LRY"
      },
      "source": [
        "指定要转换的模型的 Hugging Face 仓库 ID。它将保存到你的 Colab 文件中以供转换。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4p7yHYZ1BKh"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_author = \"\"                                         #@param {type:\"string\"}\n",
        "model_name = \"myemoji-gemma-3-270m-it\"                    #@param {type:\"string\"}\n",
        "\n",
        "repo_id = f\"{model_author}/{model_name}\"                  # 要转换的模型\n",
        "save_path = f\"/content/{model_name}\"                      # 保存调整后模型的路径\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(repo_id)     # 加载模型\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)        # 加载分词器\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"模型和分词器已保存到 {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTvHIgTT-KzZ"
      },
      "source": [
        "## 转换模型\n",
        "使用 [AI Edge Torch](https://github.com/google-ai-edge/ai-edge-torch) 转换器转换并量化模型。你可以根据任务需求调整转换参数：\n",
        "\n",
        "* `prefill_seq_len`：支持的最大输入长度\n",
        "* `kv_cache_max_len`：prefill + decode 上下文的最大长度\n",
        "* `quantize`：量化方案。8 位整数量化（INT8）适用于网页环境\n",
        "\n",
        "这个过程大约需要 10 分钟。.tflite 模型将临时保存到你的 Colab 文件中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vzTdvia1n7E"
      },
      "outputs": [],
      "source": [
        "from ai_edge_torch.generative.examples.gemma3 import gemma3\n",
        "from ai_edge_torch.generative.utilities import converter\n",
        "from ai_edge_torch.generative.utilities.export_config import ExportConfig\n",
        "from ai_edge_torch.generative.layers import kv_cache\n",
        "\n",
        "# 获取模型，设置导出配置，并转换为 .tflite\n",
        "pytorch_model = gemma3.build_model_270m(save_path)\n",
        "export_config = ExportConfig()\n",
        "export_config.kvcache_layout = kv_cache.KV_LAYOUT_TRANSPOSED\n",
        "export_config.mask_as_input = True\n",
        "converter.convert_to_tflite(\n",
        "    pytorch_model,\n",
        "    output_path=\"/content\",\n",
        "    output_name_prefix=model_name,\n",
        "    prefill_seq_len=128,\n",
        "    kv_cache_max_len=512,\n",
        "    quantize=\"dynamic_int8\",\n",
        "    export_config=export_config,\n",
        ")\n",
        "\n",
        "print (f\"模型已转换为 .tflite 并保存到 {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqFSmQxi-rCF"
      },
      "source": [
        "## 创建 MediaPipe Task 包\n",
        "\n",
        "MediaPipe Task 文件（.task）打包了原始模型分词器、LiteRT 模型（.tflite）以及使用 MediaPipe LLM 推理 API 运行端到端推理所需的附加元数据。\n",
        "\n",
        "为了使用打包器，在这一步安装 MediaPipe PyPI 包（>0.10.14），因为它有自己的依赖项。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrss72uJ-obD"
      },
      "outputs": [],
      "source": [
        "%pip install mediapipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "essOk3jeDmOV"
      },
      "source": [
        "重新安装 `protobuf` 和 `tensorflow` 并重启 Colab 运行时以获取最新版本。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LhWaLPOiUTy"
      },
      "outputs": [],
      "source": [
        "%pip uninstall protobuf -y && pip install protobuf\n",
        "%pip uninstall tensorflow -y -q && pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHC7dEhR7gFJ"
      },
      "source": [
        "现在，你将配置并创建 Task 包：\n",
        "\n",
        "1. 更新 `tflite_model` 指向你刚刚转换的 .tflite 模型\n",
        "2. 更新 `tokenizer_model` 指向从 Hugging Face Hub 下载的 tokenizer.model 文件\n",
        "3. 在 `output_filename` 中命名你的 .task 文件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OxNmqgF-VO0"
      },
      "outputs": [],
      "source": [
        "from mediapipe.tasks.python.genai import bundler\n",
        "\n",
        "config = bundler.BundleConfig(\n",
        "    tflite_model=\"/content/myemoji-gemma-3-270m-it_q8_ekv512.tflite\",     # 指向你转换的 .tflite 模型\n",
        "    tokenizer_model=\"/content/myemoji-gemma-3-270m-it/tokenizer.model\",   # 指向下载的 tokenizer.model 文件\n",
        "    start_token=\"<bos>\",\n",
        "    stop_tokens=[\"<eos>\", \"<end_of_turn>\"],\n",
        "    output_filename=\"/content/myemoji-gemma-3-270m-it.task\",              # 指定最终模型文件名\n",
        "    prompt_prefix=\"<start_of_turn>user\\n\",\n",
        "    prompt_suffix=\"<end_of_turn>\\n<start_of_turn>model\\n\",\n",
        ")\n",
        "bundler.create_bundle(config)\n",
        "\n",
        "print(f\"模型 .task 包已保存到 {config.output_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXtdT_2E5vrO"
      },
      "source": [
        "## 下载并在设备上运行模型\n",
        "\n",
        "你的模型现在已准备好用于通过 MediaPipe LLM 推理 API 在设备上运行推理！\n",
        "\n",
        "从 Colab 环境下载 .task 文件以在你的项目中使用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdfSriJW5vNk"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(config.output_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh1fQTQrcpUh"
      },
      "source": [
        "你可以在[表情符号生成网页应用](https://github.com/google-gemini/gemma-cookbook/tree/main/Demos/Emoji-Gemma-on-Web/app-mediapipe)中试用它，该应用直接在浏览器中运行模型。你也可以查看[文档](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference)以构建跨平台移动和网页应用。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Convert_Gemma_3_270M_to_LiteRT_for_MediaPipe_LLM_Inference_API.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
