{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BilmhAzR-4Qg"
      },
      "source": [
        "版权所有 2025 Google LLC。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TsslZVhvNrRy"
      },
      "outputs": [],
      "source": [
        "#@title 根据 Apache 2.0 许可证授权；\n",
        "# 除非遵守许可证，否则不得使用此文件。\n",
        "# 您可从以下地址获取许可证副本：\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0 \n",
        "#\n",
        "# 除非适用法律要求或书面同意，否则按“原样”分发软件，\n",
        "# 不附带任何明示或暗示的保证或条件。\n",
        "# 详见许可证中关于权限与限制的具体条款。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkOtOq0jDE0c"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "      <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Convert_Gemma_3_270M_to_ONNX.ipynb \"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png \" />在 Google Colab 运行</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn2YgWGrNv2M"
      },
      "source": [
        "# 将 Gemma 3 270M 转换为 ONNX\n",
        "\n",
        "本笔记本将 Gemma 3 模型导出为 ONNX 格式，以便在 [Transformers.js](https://huggingface.co/docs/transformers.js/en/index ) 中使用，后者借助 ONNX Runtime 在浏览器里运行模型。全程不到 10 分钟：\n",
        "\n",
        "1. 配置 Colab 环境\n",
        "2. 从 Hugging Face 加载模型\n",
        "3. 使用 Optimum 转换脚本导出模型\n",
        "4. 测试、评估并保存模型以供后续使用\n",
        "\n",
        "Gemma 3 270M 专为特定任务微调而设计，针对移动、网页和边缘设备的高效性能进行了优化。您可以在此[笔记本](https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Fine_tune_Gemma_3_270M_for_emoji_generation.ipynb )中微调自己的模型，转换后即可在[网页演示](https://github.com/google-gemini/gemma-cookbook/tree/main/Demos/Emoji-Gemma-on-Web/app-transformersjs )中运行。\n",
        "\n",
        "## 搭建开发环境\n",
        "\n",
        "第一步是用 pip 安装依赖包。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58TwFUM6SfFR"
      },
      "outputs": [],
      "source": [
        "%pip install transformers==4.56.1 onnx==1.19.0 onnx_ir==0.1.7 onnxruntime==1.22.1 numpy==2.3.2 huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g7wTSot_Erp"
      },
      "source": [
        "重启会话运行时，以确保使用刚安装的新版本包。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuC7NbNy9JXa"
      },
      "source": [
        "## 转换模型\n",
        "若要访问并保存模型到 Hugging Face，请提供您的[访问令牌](https://huggingface.co/settings/tokens )。可将其设为环境变量,名称填 `HF_TOKEN`，值填您的专属令牌。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-fGnIinPjdt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "login(hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0PYnb5cODbb"
      },
      "source": [
        "接下来运行 [Xenova](https://huggingface.co/Xenova ) 提供的 build_gemma.py 脚本，对 Gemma 3 模型进行转换与量化。\n",
        "\n",
        "指定待转换模型的 Hugging Face 仓库 ID，导出的 .onnx 文件将保存到您的 Colab 文件目录。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z292qdELOHOq"
      },
      "outputs": [],
      "source": [
        "!wget https://gist.githubusercontent.com/xenova/a219dbf3c7da7edd5dbb05f92410d7bd/raw/45f4c5a5227c1123efebe1e36d060672ee685a8e/build_gemma.py \n",
        "\n",
        "model_author = \"\"                                         #@param {type:\"string\"}\n",
        "gemma_model = \"myemoji-gemma-3-270m-it\"                   #@param {type:\"string\"}\n",
        "\n",
        "repo_id   = f\"{model_author}/{gemma_model}\"               # 要转换的模型\n",
        "save_path = f\"/content/{gemma_model}-onnx\"                # 保存 resized 模型的路径\n",
        "\n",
        "!python build_gemma.py \\\n",
        "    --model_name {repo_id} \\\n",
        "    --output {save_path} \\\n",
        "    -p fp32 fp16 q4 q4f16\n",
        "\n",
        "print(f\"已转换的 ONNX 模型保存在 {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40ae5e9e"
      },
      "source": [
        "## 测试转换后的模型\n",
        "\n",
        "当 .onnx 模型已保存到 Colab 会话后，可用 ONNX Runtime 测试推理。注意此处用的 ONNX Runtime 与浏览器中运行的 ONNX Runtime Web 略有差异。\n",
        "\n",
        "可在 `text_to_translate` 中尝试不同文本，并对比各量化版本的性能。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz8E9RorWWVn"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, GenerationConfig\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "# 加载配置、分词器与模型\n",
        "config = AutoConfig.from_pretrained(save_path)\n",
        "generation_config = GenerationConfig.from_pretrained(save_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
        "\n",
        "model_file = \"onnx/model.onnx\"          #@param [\"onnx/model.onnx\", \"onnx/model_fp16.onnx\", \"onnx/model_q4.onnx\", \"onnx/model_q4f16.onnx\"]\n",
        "\n",
        "model_path = f\"{save_path}/{model_file}\"\n",
        "decoder_session = onnxruntime.InferenceSession(model_path)\n",
        "\n",
        "## 设置配置值\n",
        "num_key_value_heads = config.num_key_value_heads\n",
        "head_dim = config.head_dim\n",
        "num_hidden_layers = config.num_hidden_layers\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# 准备输入\n",
        "text_to_translate = \"i love sushi\"      # @param {type:\"string\"}\n",
        "messages = [\n",
        "  { \"role\": \"system\", \"content\": \"将这段文本翻译成表情符号：\" },\n",
        "  { \"role\": \"user\", \"content\": text_to_translate },\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\")\n",
        "input_ids = inputs['input_ids']\n",
        "attention_mask = inputs['attention_mask']\n",
        "batch_size = input_ids.shape[0]\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "position_ids = np.tile(np.arange(0, input_ids.shape[-1]), (batch_size, 1))\n",
        "\n",
        "# 3. 生成循环\n",
        "max_new_tokens = 8\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "\n",
        "for i in range(max_new_tokens):\n",
        "  logits, *present_key_values = decoder_session.run(None, dict(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      position_ids=position_ids,\n",
        "      **past_key_values,\n",
        "  ))\n",
        "\n",
        "  ## 更新下一轮生成所需的值\n",
        "  input_ids = logits[:, -1].argmax(-1, keepdims=True)\n",
        "  attention_mask = np.concatenate([attention_mask, np.ones_like(input_ids, dtype=np.int64)], axis=-1)\n",
        "  position_ids = position_ids[:, -1:] + 1\n",
        "\n",
        "  for j, key in enumerate(past_key_values):\n",
        "    past_key_values[key] = present_key_values[j]\n",
        "\n",
        "  generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n",
        "\n",
        "  if np.isin(input_ids, eos_token_id).any():\n",
        "    break\n",
        "\n",
        "# 4. 输出结果\n",
        "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wVBEPJKWSEQ"
      },
      "source": [
        "## 上传到 Hugging Face Hub\n",
        "\n",
        "将导出的 ONNX 模型上传到 Hugging Face，方便分享与使用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4FNJTpKgT7I"
      },
      "outputs": [],
      "source": [
        "import huggingface_hub\n",
        "from huggingface_hub import whoami\n",
        "\n",
        "username = whoami()['name']\n",
        "\n",
        "#@markdown 给您的 ONNX 模型起个名字：\n",
        "onnx_model = \"myemoji-gemma-3-270m-it-onnx\"       #@param {type:\"string\"}\n",
        "hf_repo_id = f\"{username}/{onnx_model}\"\n",
        "\n",
        "huggingface_hub.create_repo(hf_repo_id, exist_ok=True)\n",
        "\n",
        "repo_url = huggingface_hub.upload_folder(\n",
        "  folder_path=save_path,\n",
        "  repo_id=hf_repo_id,\n",
        "  repo_type=\"model\",\n",
        "  commit_message=f\"上传 {onnx_model} 的 ONNX 模型文件\"\n",
        "  )\n",
        "\n",
        "print(f\"已上传至 {repo_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDEBKuXI_V--"
      },
      "source": [
        "## 使用 Transformers.js 部署到网页\n",
        "\n",
        "现在您可以通过 ONNX Runtime Web，借助 [Transformers.js](https://huggingface.co/docs/transformers.js/en/index ) 在浏览器中运行 Gemma 3 模型。可在[表情符号生成网页演示](https://github.com/google-gemini/gemma-cookbook/tree/main/Demos/Emoji-Gemma-on-Web/app-transformersjs )中体验，模型完全在浏览器端运行。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Convert_Gemma_3_270M_to_ONNX.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
